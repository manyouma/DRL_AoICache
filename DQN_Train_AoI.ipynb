{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "#from itertools import count\n",
    "import scipy.io as sio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "GOAL = 250\n",
    "NUM_FILES = 1\n",
    "DELTA_VALUE = 5\n",
    "NUM_STATE = NUM_FILES*(DELTA_VALUE+1)\n",
    "A_HAT = 50\n",
    "N_USERS = 2\n",
    "ETA_value = 5\n",
    "RANDOM_SEED = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "random.seed(RANDOM_SEED)\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.inlayer = nn.Linear(inputs, 64)\n",
    "        self.hidlayer1 = nn.Linear(64, 32)\n",
    "        self.hidlayer2 = nn.Linear(32, 16)\n",
    "        self.outlayer = nn.Linear(16, outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.inlayer(x.view(x.size(0), -1)))\n",
    "        x = F.relu(self.hidlayer1(x))\n",
    "        x = F.relu(self.hidlayer2(x))\n",
    "        return self.outlayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.0\n",
    "EPS_DECAY = 100\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "n_actions = NUM_FILES+1\n",
    "\n",
    "policy_net = DQN(NUM_STATE, n_actions).to(device)\n",
    "target_net = DQN(NUM_STATE, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(reference_state):\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    " \n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values = target_net(next_state_batch).max(1)[0].detach() \n",
    "    expected_state_action_values = next_state_values + reward_batch - target_net(reference_state).max(1)[0].detach()\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AoI_simulator:\n",
    "\n",
    "    def __init__(self, N_files=1, A_hat=50, eta=20, Delta= 4, P_arrive= 0.3, N_users = 4):\n",
    "        self.N_files = N_files\n",
    "        self.A_hat = A_hat \n",
    "        self.eta = eta\n",
    "        self.Delta = Delta\n",
    "        self.P_arrive = P_arrive\n",
    "        self.N_users = N_users\n",
    "        self.current_AoI = np.ones([self.N_files,1])\n",
    "        self.current_queue = np.zeros([self.N_files, self.Delta])\n",
    "        \n",
    "        for i_file in range(self.N_files):\n",
    "            self.current_queue[i_file,-1] = np.random.binomial(self.N_users, self.P_arrive)            \n",
    "        self.state = np.concatenate((self.current_AoI.reshape((1,-1)), self.current_queue.reshape((1,-1))), axis=1)\n",
    "\n",
    "  \n",
    "    def step(self, action):        \n",
    "        reward = 0 \n",
    "        next_AoI = np.zeros_like(self.current_AoI)\n",
    "        next_queue = np.zeros_like(self.current_queue)\n",
    "\n",
    "        for i_file in range(self.N_files):\n",
    "            next_queue[i_file,:-1] = self.current_queue[i_file,1:]\n",
    "            next_queue[i_file, -1] = np.random.binomial(self.N_users, self.P_arrive)\n",
    "            reward = reward + self.current_AoI[i_file]*self.current_queue[i_file,0]/(self.P_arrive*N_USERS)\n",
    "\n",
    "            if action == i_file+1:\n",
    "                reward = reward + self.eta\n",
    "                next_AoI[i_file] = 1 \n",
    "            else:\n",
    "                next_AoI[i_file] = min(self.A_hat, self.current_AoI[i_file]+1)\n",
    "        \n",
    "        self.current_AoI = next_AoI\n",
    "        self.current_queue = next_queue\n",
    "        \n",
    "        self.state = np.concatenate((self.current_AoI.reshape((1,-1)), self.current_queue.reshape((1,-1))), axis=1)\n",
    "        \n",
    "        return (self.state, -reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.844444444444448\n",
      "-21.094444444444445\n",
      "-2.827777777777776\n",
      "-2.7833333333333337\n",
      "-3.1055555555555556\n",
      "-2.822222222222222\n",
      "-2.938888888888888\n",
      "-2.9333333333333322\n",
      "-2.86111111111111\n",
      "-3.2388888888888903\n",
      "-3.183333333333333\n",
      "-2.938888888888888\n",
      "-3.0888888888888886\n",
      "-3.1333333333333324\n",
      "-2.994444444444444\n",
      "-3.2277777777777774\n",
      "-3.222222222222223\n",
      "-3.161111111111111\n",
      "-3.005555555555556\n",
      "-2.8722222222222222\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 200\n",
    "T = 300\n",
    "total_reward = 0\n",
    "epi_val = np.zeros(num_episodes)\n",
    "for i_episode in range(num_episodes):\n",
    "    total_reward = 0\n",
    "    aoi_sim = AoI_simulator(N_files=NUM_FILES, Delta=DELTA_VALUE, A_hat=A_HAT, P_arrive = 0.3, N_users = N_USERS, eta = ETA_value)\n",
    "    state = aoi_sim.state.squeeze()\n",
    "    \n",
    "    state = torch.tensor([state], dtype=torch.float32)\n",
    "    state = state.to(device)\n",
    "    reference_state = state\n",
    "    \n",
    "    for t in range(T):\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward  = aoi_sim.step(action.item())\n",
    "        reward = reward.squeeze()\n",
    "        next_state = next_state.squeeze()\n",
    "        total_reward += reward\n",
    "        next_state = torch.tensor([next_state], dtype=torch.float32)\n",
    "        next_state = next_state.to(device)\n",
    "        reward = torch.tensor([1*reward],dtype=torch.float32, device=device)\n",
    "        \n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        optimize_model(reference_state) \n",
    "        \n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(total_reward/(T*NUM_FILES))\n",
    "    epi_val[i_episode] = total_reward/(T*NUM_FILES)\n",
    "print('Complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10000\n",
    "total_reward = 0\n",
    "AoI_vec = np.zeros([NUM_FILES, T])\n",
    "request_vec = np.zeros(T)\n",
    "update_vec = np.zeros(T)\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "aoi_sim = AoI_simulator(N_files=NUM_FILES, Delta=DELTA_VALUE, A_hat=A_HAT, P_arrive = (0.3), N_users = N_USERS, eta = ETA_value)\n",
    "state = aoi_sim.state.squeeze()\n",
    "    # Initialize the environment and state with random starting action\n",
    "state = torch.tensor([state], dtype=torch.float32) # numpy array to torch tensor\n",
    "state = state.to(device)\n",
    "    \n",
    "    \n",
    "for t in range(T):\n",
    "        # Select and perform an action\n",
    "    action = select_action(state)\n",
    "    next_state,reward  = aoi_sim.step(action.item())\n",
    "    next_state = next_state.squeeze()\n",
    "    next_state = torch.tensor([next_state], dtype=torch.float32)\n",
    "    next_state = next_state.to(device)\n",
    "    state = next_state\n",
    "    AoI_vec[:,t] = aoi_sim.current_AoI.squeeze()*aoi_sim.current_queue[:,0].squeeze()\n",
    "    request_vec[t] = np.sum(aoi_sim.current_queue[:,0])\n",
    "    update_vec[t] = action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_aoi = np.sum(AoI_vec)/np.sum(request_vec)\n",
    "mu = np.sum(update_vec>0)/T\n",
    "total_cost = average_aoi + ETA_value*mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "adict = {}\n",
    "adict['convergence_vec'] = epi_val\n",
    "adict['total_cost'] = total_cost\n",
    "adict['average_aoi'] = average_aoi\n",
    "adict['mu'] = mu\n",
    "\n",
    "file_name = 'AoI_Eta%d_Random%d_Delta%d_N_User%d.mat'%(ETA_value,RANDOM_SEED,DELTA_VALUE,N_USERS)\n",
    "sio.savemat(file_name, adict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
